{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Path to Yu group's github repo\n",
    "YU_REPO = Path('../data/external/covid19-severity-prediction/')\n",
    "\n",
    "# Add their modules to python path\n",
    "sys.path.append(str(YU_REPO))\n",
    "\n",
    "# Path to our data folder\n",
    "DATA = Path('../data/')\n",
    "\n",
    "# Flag to indicate whether to use cached or uncached files\n",
    "cached=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import data\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded and merged COVID-19 cases/deaths data successfully\n"
     ]
    }
   ],
   "source": [
    "# Read data (returns wide format)\n",
    "# df_unabridged = data.load_county_data(data_dir = str(YU_REPO/'data'), cached = cached, abridged = False)\n",
    "df_abridged = data.load_county_data(data_dir = str(YU_REPO/'data'), cached = cached, abridged = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abridged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_abridged['countyFIPS'].nunique() == df_abridged.shape[0], \"Non unique values for countyFIPS\"\n",
    "\n",
    "# Set FIPS to be the index\n",
    "df_abridged.set_index('countyFIPS', inplace=True, drop=True)\n",
    "\n",
    "# Mapping from county FIPS to CountyName and State\n",
    "countyfips2identifier = df_abridged.loc[:, ['STATEFP', 'COUNTYFP', 'CountyName', 'StateName', 'State']].T.to_dict()\n",
    "\n",
    "# Delete redundant identifiers \n",
    "df_abridged.drop(['STATEFP', 'COUNTYFP', 'CountyName', 'State'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stationary features\n",
    "stationary_features = open(DATA/'interim/abridged_stationary_feature_list.txt', 'r').read().split('\\n')\n",
    "\n",
    "# Subset dataframe and save to disk\n",
    "df_abridged[stationary_features].to_csv(DATA/'processed/abridged_stationary_features.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Remove\n",
    "df_abridged.drop(stationary_features, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time varying features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cases and Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time varying features\n",
    "time_varying_features = list()\n",
    "\n",
    "# County wise date of first death\n",
    "first_death_date = dict()\n",
    "\n",
    "# How far back to look \n",
    "K = 5\n",
    "\n",
    "deathcols = df_abridged.columns.values[np.where(df_abridged.columns.str.startswith('#Deaths'))[0]]\n",
    "casecols =  df_abridged.columns.values[np.where(df_abridged.columns.str.startswith('#Case'))[0]]\n",
    "\n",
    "\n",
    "for county, row in df_abridged.iterrows():\n",
    "\n",
    "    # Skip counties which have not had any deaths\n",
    "    if row['deaths'].max() == 0: continue\n",
    "        \n",
    "    # Find the first day of death\n",
    "    first_death_day = np.where(np.array(row['deaths']) > 0)[0][0]        \n",
    "    \n",
    "    # Index according to the common time scale (days since first death)\n",
    "    modts_deaths = row['deaths'][first_death_day:]\n",
    "    modts_cases = row['cases'][first_death_day:]\n",
    "    \n",
    "    # Skip counties which have not had K days since first death\n",
    "    if len(modts_deaths) < (K+1): continue \n",
    "    \n",
    "    # Subset from day of first day of death onwards\n",
    "    datewise_deaths = row[deathcols[first_death_day:]]\n",
    "    datewise_cases = row[casecols[first_death_day:]]\n",
    "    \n",
    "    for i, (lag_deaths, lag_cases) in enumerate(zip(ngrams(datewise_deaths, K+1), ngrams(datewise_cases, K+1))):\n",
    "        features = {'countyFIPS': county}\n",
    "        for j in range(K+1):\n",
    "            features[f'deaths-{j}'] = lag_deaths[K-j]\n",
    "            features[f'cases-{j}'] = lag_cases[K-j]\n",
    "            features['days_since_first_death'] = K+i\n",
    "            \n",
    "        time_varying_features.append(features)\n",
    "        \n",
    "    first_death_date[county] = dt.strptime(deathcols[first_death_day][8:], '%m-%d-%Y')\n",
    "    \n",
    "        \n",
    "first_death_date = pd.DataFrame(first_death_date.items(), columns=['countyFIPS', 'first_death_date']).set_index('countyFIPS')\n",
    "\n",
    "# Save to disk\n",
    "pd.DataFrame(time_varying_features).to_csv(DATA/'processed/abridged_time_varying_features.tsv', sep='\\t', \n",
    "                                           encoding='utf-8', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time based feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_based_features = df_abridged[['stay at home', '>50 gatherings', '>500 gatherings', 'public schools',\n",
    "                                   'restaurant dine-in', 'entertainment/gym', 'federal guidelines']]\n",
    "\n",
    "# Remove counties which have not experienced any deaths or K days since first death\n",
    "time_based_features = time_based_features.loc[first_death_date.index]\n",
    "\n",
    "# Convert to datetime\n",
    "time_based_features = time_based_features.applymap(lambda s: dt.fromordinal(int(s)) if s==s else s)\n",
    "\n",
    "for col in time_based_features.columns:\n",
    "    time_based_features[col] = (time_based_features[col] - first_death_date['first_death_date'])\n",
    "    \n",
    "    # Convert to integer\n",
    "    time_based_features[col] =  time_based_features[col].map(lambda s: s.days if s==s else np.nan)\n",
    "    \n",
    "# Write to disk\n",
    "time_based_features.to_csv(DATA/'processed/abridged_time_based_features.tsv', sep='\\t', \n",
    "                           encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_covid",
   "language": "python",
   "name": "py38_covid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
