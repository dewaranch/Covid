{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Path to Yu group's github repo\n",
    "YU_REPO = Path('../data/external/covid19-severity-prediction/')\n",
    "\n",
    "# Add their modules to python path\n",
    "sys.path.append(str(YU_REPO))\n",
    "\n",
    "# Path to our data folder\n",
    "DATA = Path('../data/')\n",
    "\n",
    "# Flag to indicate whether to use cached or uncached files\n",
    "cached=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from nltk import ngrams\n",
    "import addfips\n",
    "af = addfips.AddFIPS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded and merged COVID-19 cases/deaths data successfully\n"
     ]
    }
   ],
   "source": [
    "# Read data (returns wide format)\n",
    "# df_unabridged = data.load_county_data(data_dir = str(YU_REPO/'data'), cached = cached, abridged = False)\n",
    "df_abridged = data.load_county_data(data_dir = str(YU_REPO/'data'), cached = cached, abridged = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from state abbreviation to full name\n",
    "state_abb = pd.read_csv(DATA/'interim/state-abbreviations.csv', header=None)\n",
    "state_abb = state_abb.set_index(1)[0].to_dict()\n",
    "\n",
    "# Fill null state names in abridged data\n",
    "df_abridged['State'] = df_abridged['StateName'].map(lambda s: state_abb[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abridged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_abridged['countyFIPS'].nunique() == df_abridged.shape[0], \"Non unique values for countyFIPS\"\n",
    "\n",
    "# Set FIPS to be the index\n",
    "df_abridged.set_index('countyFIPS', inplace=True, drop=True)\n",
    "\n",
    "# Mapping from county FIPS to CountyName and State\n",
    "countyfips2identifier = df_abridged.loc[:, ['STATEFP', 'COUNTYFP', 'CountyName', 'StateName', 'State']].T.to_dict()\n",
    "\n",
    "# Delete redundant identifiers \n",
    "df_abridged.drop(['STATEFP', 'COUNTYFP', 'CountyName', 'State'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stationary features\n",
    "stationary_features = open(DATA/'interim/abridged_stationary_feature_list.txt', 'r').read().split('\\n')\n",
    "\n",
    "# Subset dataframe and save to disk\n",
    "df_abridged[stationary_features].to_csv(DATA/'processed/abridged_stationary_features.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Remove\n",
    "df_abridged.drop(stationary_features, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time varying features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cases and Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time varying features\n",
    "time_varying_features = list()\n",
    "\n",
    "# County wise date of first death\n",
    "first_death_date = dict()\n",
    "\n",
    "# How far back to look \n",
    "K = 5\n",
    "\n",
    "deathcols = df_abridged.columns.values[np.where(df_abridged.columns.str.startswith('#Deaths'))[0]]\n",
    "casecols =  df_abridged.columns.values[np.where(df_abridged.columns.str.startswith('#Case'))[0]]\n",
    "\n",
    "for county, row in df_abridged.iterrows():\n",
    "\n",
    "    # Skip counties which have not had any deaths\n",
    "    if row['deaths'].max() == 0: continue\n",
    "        \n",
    "    # Find the first day of death\n",
    "    first_death_day = np.where(np.array(row['deaths']) > 0)[0][0]        \n",
    "    \n",
    "    # Index according to the common time scale (days since first death)\n",
    "    modts_deaths = row['deaths'][first_death_day:]\n",
    "    modts_cases = row['cases'][first_death_day:]\n",
    "    \n",
    "    # Skip counties which have not had K days since first death\n",
    "    if len(modts_deaths) < (K+1): continue \n",
    "    \n",
    "    # Subset from day of first day of death onwards\n",
    "    datewise_deaths = row[deathcols[first_death_day:]]\n",
    "    datewise_cases = row[casecols[first_death_day:]]\n",
    "    \n",
    "    for i, (lag_deaths, lag_cases) in enumerate(zip(ngrams(datewise_deaths, K+1), ngrams(datewise_cases, K+1))):\n",
    "        features = {'countyFIPS': county}\n",
    "        for j in range(K+1):\n",
    "            features[f'deaths-{j}'] = lag_deaths[K-j]\n",
    "            features[f'cases-{j}'] = lag_cases[K-j]\n",
    "            features['days_since_first_death'] = K+i\n",
    "            \n",
    "        time_varying_features.append(features)\n",
    "        \n",
    "    first_death_date[county] = dt.strptime(deathcols[first_death_day][8:], '%m-%d-%Y')\n",
    "    \n",
    "        \n",
    "first_death_date = pd.DataFrame(first_death_date.items(), columns=['countyFIPS', 'first_death_date']).set_index('countyFIPS')\n",
    "\n",
    "# Save to disk\n",
    "pd.DataFrame(time_varying_features).to_csv(DATA/'processed/abridged_time_varying_features.tsv', sep='\\t', \n",
    "                                           encoding='utf-8', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17 countyFIPS not present in mobility dataset.\n"
     ]
    }
   ],
   "source": [
    "google_mobility = pd.read_csv(DATA/'external/Global_Mobility_Report.csv', low_memory=False)\n",
    "\n",
    "# Drop rows with missing info for sub_region_1 or sub_region_2\n",
    "google_mobility.dropna(subset=['sub_region_1', 'sub_region_2'], inplace=True)\n",
    "\n",
    "# Keep only data for US\n",
    "google_mobility = google_mobility[google_mobility['country_region'] == 'United States']\n",
    "\n",
    "# Add countyFIPS\n",
    "google_mobility['countyFIPS'] = google_mobility.\\\n",
    "    apply(lambda row: af.get_county_fips(row['sub_region_2'], state=row['sub_region_1']), axis=1)\n",
    "\n",
    "# Ensure evereything got mapped \n",
    "assert google_mobility['countyFIPS'].map(len).min() == 5, \"Some FIPS codes are wrong\"\n",
    "\n",
    "# Set index to county fips\n",
    "google_mobility.set_index('countyFIPS', inplace=True, drop=True)\n",
    "\n",
    "# Convert to datetime\n",
    "google_mobility['date'] = google_mobility['date'].map(lambda s: dt.strptime(s, \"%Y-%m-%d\"))\n",
    "\n",
    "# Only keep counties which for which have experienced K days since first death and present in mobility dataset\n",
    "common_fips = list(set(first_death_date.index) & set(google_mobility.index))\n",
    "missing_fips = set(first_death_date.index) - set(google_mobility.index)\n",
    "print(f\"There are {len(missing_fips)} countyFIPS not present in mobility dataset.\")\n",
    "google_mobility = google_mobility.loc[common_fips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to days since first death\n",
    "countyFIPS2first_death_date = first_death_date.to_dict()['first_death_date']\n",
    "google_mobility['days_since_first_death'] = google_mobility.reset_index().apply(\n",
    "    lambda row: (row['date'] - countyFIPS2first_death_date[row['countyFIPS']]).days, axis=1).values\n",
    "\n",
    "# Remove redundant identifiers\n",
    "google_mobility.drop(['country_region_code', 'country_region', 'sub_region_1', 'sub_region_2', 'date'], axis=1, inplace=True)\n",
    "\n",
    "# Groupby county FIPS\n",
    "google_mobility = google_mobility.reset_index().groupby('countyFIPS').agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add google mobility to time varying features dataset\n",
    "K = 6\n",
    "time_varying_features = pd.DataFrame(time_varying_features)\n",
    "mobility_colnames = ['retail_and_recreation_percent_change_from_baseline',\n",
    "                     'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                     'parks_percent_change_from_baseline',\n",
    "                     'transit_stations_percent_change_from_baseline',\n",
    "                     'workplaces_percent_change_from_baseline',\n",
    "                     'residential_percent_change_from_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_features = list() \n",
    "\n",
    "for i, row in time_varying_features.iterrows():\n",
    "    \n",
    "    features = {\n",
    "        'countyFIPS': row['countyFIPS'],\n",
    "        'days_since_first_death': row['days_since_first_death']\n",
    "    }\n",
    "    \n",
    "    # Retrieve mobility data for that specific county\n",
    "    try:\n",
    "        county_mobility = google_mobility.loc[features['countyFIPS']]\n",
    "        \n",
    "    # No mobility data at all for that county (this raises valueerror later and thus everything gets set to null)\n",
    "    except KeyError: \n",
    "        county_mobility = {'days_since_first_death': [-1e9]}\n",
    "    \n",
    "    # Find mobility data preceding the days since first death\n",
    "    try:\n",
    "        idx = county_mobility['days_since_first_death'].index(features['days_since_first_death'])\n",
    "\n",
    "        for col in mobility_colnames:\n",
    "            for j in range(K):   \n",
    "                features[f'{col}-{j}']  = county_mobility[col][idx-j]\n",
    "    \n",
    "    # Mobility data doesn't exist a specific date onwards\n",
    "    except ValueError:\n",
    "        for col in mobility_colnames:\n",
    "            for j in range(K):   \n",
    "                features[f'{col}-{j}'] = np.nan\n",
    "        \n",
    "    mobility_features.append(features)\n",
    "    \n",
    "mobility_features = pd.DataFrame(mobility_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_features.to_csv(DATA/'processed/mobility_time_varying_features.tsv', sep='\\t', \n",
    "                                       encoding='utf-8', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time based feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_based_features = df_abridged[['stay at home', '>50 gatherings', '>500 gatherings', 'public schools',\n",
    "                                   'restaurant dine-in', 'entertainment/gym', 'federal guidelines']]\n",
    "\n",
    "# Remove counties which have not experienced any deaths or K days since first death\n",
    "time_based_features = time_based_features.loc[first_death_date.index]\n",
    "\n",
    "# Convert to datetime\n",
    "time_based_features = time_based_features.applymap(lambda s: dt.fromordinal(int(s)) if s==s else s)\n",
    "\n",
    "for col in time_based_features.columns:\n",
    "    time_based_features[col] = (time_based_features[col] - first_death_date['first_death_date'])\n",
    "    \n",
    "    # Convert to integer\n",
    "    time_based_features[col] =  time_based_features[col].map(lambda s: s.days if s==s else np.nan)\n",
    "    \n",
    "# Write to disk\n",
    "time_based_features.to_csv(DATA/'processed/abridged_time_based_features.tsv', sep='\\t', \n",
    "                           encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_covid",
   "language": "python",
   "name": "py38_covid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
